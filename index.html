<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Installation" href="installation.html" />

    <!-- Generated with Sphinx 6.1.3 and Furo 2022.12.07 -->
        <title>speakerbox 0.1.dev1+g0771d72 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="#"><div class="brand">speakerbox 0.1.dev1+g0771d72 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="#">
  
  
  <span class="sidebar-brand-text">speakerbox 0.1.dev1+g0771d72 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="modules.html">Package modules</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="speakerbox.html">speakerbox package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="welcome-to-speakerbox-s-documentation">
<h1>Welcome to speakerbox’s documentation!<a class="headerlink" href="#welcome-to-speakerbox-s-documentation" title="Permalink to this heading">#</a></h1>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="speakerbox">
<h1>speakerbox<a class="headerlink" href="#speakerbox" title="Permalink to this heading">#</a></h1>
<a class="reference external image-reference" href="https://github.com/CouncilDataProject/speakerbox/actions"><img alt="Build Status" src="https://github.com/CouncilDataProject/speakerbox/workflows/CI/badge.svg" /></a>
<a class="reference external image-reference" href="https://CouncilDataProject.github.io/speakerbox"><img alt="Documentation" src="https://github.com/CouncilDataProject/speakerbox/workflows/Documentation/badge.svg" /></a>
<a class="reference external image-reference" href="https://joss.theoj.org/papers/49cfcef1769c812ce4ff2e388a5c7641"><img alt="status" src="https://joss.theoj.org/papers/49cfcef1769c812ce4ff2e388a5c7641/status.svg" /></a>
<p>Few-Shot Multi-Recording Speaker Identification Transformer Fine-Tuning and Application</p>
<hr class="docutils" />
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">#</a></h2>
<p><strong>Stable Release:</strong> <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">speakerbox</span></code><span class="raw-html-m2r"><br></span>
<strong>Development Head:</strong> <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">git+https://github.com/CouncilDataProject/speakerbox.git</span></code></p>
</section>
<section id="documentation">
<h2>Documentation<a class="headerlink" href="#documentation" title="Permalink to this heading">#</a></h2>
<p>For full package documentation please visit <a class="reference external" href="https://councildataproject.github.io/speakerbox">councildataproject.github.io/speakerbox</a>.</p>
</section>
<section id="example-usage-video">
<h2>Example Usage Video<a class="headerlink" href="#example-usage-video" title="Permalink to this heading">#</a></h2>
<a class="reference external image-reference" href="https://youtu.be/SK2oVqSKPTE"><img alt="screenshot from example usage youtube video" src="https://raw.githubusercontent.com/CouncilDataProject/speakerbox/main/docs/_static/images/speakerbox-example-video-screenshot.png" /></a>
<p>Link: <a class="reference external" href="https://youtu.be/SK2oVqSKPTE">https://youtu.be/SK2oVqSKPTE</a></p>
<p>In the example video, we use the Speakerbox library to quickly annotate a
dataset of audio clips from the show
<a class="reference external" href="https://en.wikipedia.org/wiki/The_West_Wing">The West Wing</a>
and train a speaker identification model to identify three of
the show’s characters (President Bartlet, Charlie Young, and Leo McGarry).</p>
</section>
<section id="problem">
<h2>Problem<a class="headerlink" href="#problem" title="Permalink to this heading">#</a></h2>
<p>Given a set of recordings of multi-speaker recordings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>example/
├── 0.wav
├── 1.wav
├── 2.wav
├── 3.wav
├── 4.wav
└── 5.wav
</pre></div>
</div>
<p>Where each recording has some or all of a set of speakers, for example:</p>
<ul class="simple">
<li><p>0.wav – contains speakers: A, B, C</p></li>
<li><p>1.wav – contains speakers: A, C</p></li>
<li><p>2.wav – contains speakers: B, C</p></li>
<li><p>3.wav – contains speakers: A, B, C</p></li>
<li><p>4.wav – contains speakers: A, B, C</p></li>
<li><p>5.wav – contains speakers: A, B, C</p></li>
</ul>
<p>You want to train a model to classify portions of audio as one of the N known speakers
in future recordings not included in your original training set.</p>
<p><code class="docutils literal notranslate"><span class="pre">f(audio)</span> <span class="pre">-&gt;</span> <span class="pre">[(start_time,</span> <span class="pre">end_time,</span> <span class="pre">speaker),</span> <span class="pre">(start_time,</span> <span class="pre">end_time,</span> <span class="pre">speaker),</span> <span class="pre">...]</span></code></p>
<p>i.e. <code class="docutils literal notranslate"><span class="pre">f(audio)</span> <span class="pre">-&gt;</span> <span class="pre">[(2.4,</span> <span class="pre">10.5,</span> <span class="pre">&quot;A&quot;),</span> <span class="pre">(10.8,</span> <span class="pre">14.1,</span> <span class="pre">&quot;D&quot;),</span> <span class="pre">(14.8,</span> <span class="pre">22.7,</span> <span class="pre">&quot;B&quot;),</span> <span class="pre">...]</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">speakerbox</span></code> library contains methods for both generating datasets for annotation
and for utilizing multiple audio annotation schemes to train such a model.</p>
<a class="reference external image-reference" href="https://raw.githubusercontent.com/CouncilDataProject/speakerbox/main/docs/_static/images/workflow.png"><img alt="Typical workflow to prepare a speaker identification dataset and fine-tune a new model using tools provided from the Speakerbox library. The user starts with a collection of audio files that include portions speech from the speakers they want to train a model to identify. The `diarize_and_split_audio` function will create a new directory with the same name as the audio file, diarize the audio file, and finally, sort the audio portions produced from diarization into sub-directories within this new directory. The user should then manually rename each of the produced sub-directories to the correct speaker identifier (i.e. the speaker's name or a unique id) and additionally remove any incorrectly diarized or mislabeled portions of audio. Finally, the user can prepare training, evaluation, and testing datasets (via the `expand_labeled_diarized_audio_dir_to_dataset` and `preprocess_dataset` functions) and fine-tune a new speaker identification model (via the `train` function)." src="https://raw.githubusercontent.com/CouncilDataProject/speakerbox/main/docs/_static/images/workflow.png" /></a>
<p>The following table shows model performance results as the dataset size increases:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>dataset_size</p></th>
<th class="head"><p>mean_accuracy</p></th>
<th class="head"><p>mean_precision</p></th>
<th class="head"><p>mean_recall</p></th>
<th class="head"><p>mean_training_duration_seconds</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>15-minutes</p></td>
<td><p>0.874 ± 0.029</p></td>
<td><p>0.881 ± 0.037</p></td>
<td><p>0.874 ± 0.029</p></td>
<td><p>101 ± 1</p></td>
</tr>
<tr class="row-odd"><td><p>30-minutes</p></td>
<td><p>0.929 ± 0.006</p></td>
<td><p>0.94 ± 0.007</p></td>
<td><p>0.929 ± 0.006</p></td>
<td><p>186 ± 3</p></td>
</tr>
<tr class="row-even"><td><p>60-minutes</p></td>
<td><p>0.937 ± 0.02</p></td>
<td><p>0.94 ± 0.017</p></td>
<td><p>0.937 ± 0.02</p></td>
<td><p>453 ± 7</p></td>
</tr>
</tbody>
</table>
</div>
<p>All results reported are the average of five model training and evaluation trials for each
of the different dataset sizes. All models were fine-tuned using an NVIDIA GTX 1070 TI.</p>
<p><strong>Note:</strong> this table can be reproduced in ~1 hour using an NVIDIA GTX 1070 TI by:</p>
<p>Installing the example data download dependency:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>speakerbox<span class="o">[</span>example_data<span class="o">]</span>
</pre></div>
</div>
<p>Then running the following commands in Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox.examples</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">download_preprocessed_example_data</span><span class="p">,</span>
    <span class="n">train_and_eval_all_example_models</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Download and unpack the preprocessed example data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">download_preprocessed_example_data</span><span class="p">()</span>

<span class="c1"># Train and eval models with different subsets of the data</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train_and_eval_all_example_models</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Permalink to this heading">#</a></h2>
<section id="diarization">
<h3>Diarization<a class="headerlink" href="#diarization" title="Permalink to this heading">#</a></h3>
<p>We quickly generate an annotated dataset by first diarizing (or clustering based
on the features of speaker audio) portions of larger audio files and splitting each the
of the clusters into their own directories that you can then manually clean up
(by removing incorrectly clustered audio segments).</p>
<section id="notes">
<h4>Notes<a class="headerlink" href="#notes" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>It is recommended to have each larger audio file named with a unique id that
can be used to act as a “recording id”.</p></li>
<li><p>Diarization time depends on machine resources and make take a long time – one
potential recommendation is to run a diarization script overnight and clean up the
produced annotations the following day.</p></li>
<li><p>During this process audio will be duplicated in the form of smaller audio clips –
ensure you have enough space on your machine to complete this process before
you begin.</p></li>
<li><p>Clustering accuracy depends on how many speakers there are, how distinct their
voices are, and how much speech is talking over one-another.</p></li>
<li><p>If possible, try to find recordings where speakers have a roughly uniform distribution
of speaking durations.</p></li>
</ul>
<p>⚠️ To use the diarization portions of <code class="docutils literal notranslate"><span class="pre">speakerbox</span></code> you need to complete the
following steps: ⚠️</p>
<ol class="arabic simple">
<li><p>Visit <a class="reference external" href="https://hf.co/pyannote/speaker-diarization">hf.co/pyannote/speaker-diarization</a>
and accept user conditions.</p></li>
<li><p>Visit <a class="reference external" href="https://hf.co/pyannote/segmentation">hf.co/pyannote/segmentation</a>
and accept user conditions.</p></li>
<li><p>Visit <a class="reference external" href="https://hf.co/settings/tokens">hf.co/settings/tokens</a> to create an access token
(only if you had to complete 1.)</p></li>
</ol>
<p><strong>Diarize a single file:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox</span> <span class="kn">import</span> <span class="n">preprocess</span>

<span class="c1"># The token can also be provided via the &#39;HUGGINGFACE_TOKEN` environment variable.</span>
<span class="n">diarized_and_split_audio_dir</span> <span class="o">=</span> <span class="n">preprocess</span><span class="o">.</span><span class="n">diarize_and_split_audio</span><span class="p">(</span>
    <span class="s2">&quot;0.wav&quot;</span><span class="p">,</span>
    <span class="n">hf_token</span><span class="o">=</span><span class="s2">&quot;token-from-hugging-face&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Diarize all files in a directory:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox</span> <span class="kn">import</span> <span class="n">preprocess</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Iterate over all &#39;wav&#39; format files in a directory called &#39;data&#39;</span>
<span class="k">for</span> <span class="n">audio_file</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*.wav&quot;</span><span class="p">))):</span>
    <span class="c1"># The token can also be provided via the &#39;HUGGINGFACE_TOKEN` environment variable.</span>
    <span class="n">diarized_and_split_audio_dir</span> <span class="o">=</span> <span class="n">preprocess</span><span class="o">.</span><span class="n">diarize_and_split_audio</span><span class="p">(</span>
        <span class="n">audio_file</span><span class="p">,</span>
        <span class="c1"># Create a new directory to place all created sub-directories within</span>
        <span class="n">storage_dir</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;diarized-audio/</span><span class="si">{</span><span class="n">audio_file</span><span class="o">.</span><span class="n">stem</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">hf_token</span><span class="o">=</span><span class="s2">&quot;token-from-hugging-face&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="cleaning">
<h3>Cleaning<a class="headerlink" href="#cleaning" title="Permalink to this heading">#</a></h3>
<p>Diarization will produce a directory structure organized by unlabeled speakers with
the audio clips that were clustered together.</p>
<p>For example, if <code class="docutils literal notranslate"><span class="pre">&quot;0.wav&quot;</span></code> had three speakers, the produced directory structure may look
like the following tree:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>0/
├── SPEAKER_00
│   ├── 567-12928.wav
│   ├── ...
│   └── 76192-82901.wav
├── SPEAKER_01
│   ├── 34123-38918.wav
│   ├── ...
│   └── 88212-89111.wav
└── SPEAKER_02
    ├── ...
    └── 53998-62821.wav
</pre></div>
</div>
<p>We leave it to you as a user to then go through these directories and remove any audio
clips that were incorrectly clustered together as well as renaming the sub-directories
to their correct speaker labels. For example, labelled sub-directories may look like
the following tree:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>0/
├── A
│   ├── 567-12928.wav
│   ├── ...
│   └── 76192-82901.wav
├── B
│   ├── 34123-38918.wav
│   ├── ...
│   └── 88212-89111.wav
└── D
    ├── ...
    └── 53998-62821.wav
</pre></div>
</div>
<section id="id1">
<h4>Notes<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Most operating systems have an audio playback application to queue an entire directory
of audio files as a playlist for playback. This makes it easy to listen to a whole
unlabeled sub-directory (i.e. “SPEAKER_00”) at a time and pause playback and remove
files from the directory which were incorrectly clustered.</p></li>
<li><p>If any clips have overlapping speakers, it is up to you as a user if you want to
remove those clips or keep them and properly label them with the speaker you wish to
associate them with.</p></li>
</ul>
</section>
</section>
<section id="training-preparation">
<h3>Training Preparation<a class="headerlink" href="#training-preparation" title="Permalink to this heading">#</a></h3>
<p>Once you have annotated what you think is enough recordings, you can try preparing
a dataset for training.</p>
<p>The following functions will prepare the audio for training by:</p>
<ol class="arabic simple">
<li><p>Finding all labeled audio clips in the provided directories</p></li>
<li><p>Chunk all found audio clips into smaller duration clips <em>(parametrizable)</em></p></li>
<li><p>Check that the provided annotated dataset meets the following conditions:</p>
<ol class="arabic simple">
<li><p>There is enough data such that the training, test, and validation subsets all
contain different recording ids.</p></li>
<li><p>There is enough data such that the training, test, and validation subsets each
contain all labels present in the whole dataset.</p></li>
</ol>
</li>
</ol>
<section id="id2">
<h4>Notes<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>During this process audio will be duplicated in the form of smaller audio clips –
ensure you have enough space on your machine to complete this process before
you begin.</p></li>
<li><p>Directory names are used as recording ids during dataset construction.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox</span> <span class="kn">import</span> <span class="n">preprocess</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">preprocess</span><span class="o">.</span><span class="n">expand_labeled_diarized_audio_dir_to_dataset</span><span class="p">(</span>
    <span class="n">labeled_diarized_audio_dir</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;0/&quot;</span><span class="p">,</span>  <span class="c1"># The cleaned and checked audio clips for recording id 0</span>
        <span class="s2">&quot;1/&quot;</span><span class="p">,</span>  <span class="c1"># ... recording id 1</span>
        <span class="s2">&quot;2/&quot;</span><span class="p">,</span>  <span class="c1"># ... recording id 2</span>
        <span class="s2">&quot;3/&quot;</span><span class="p">,</span>  <span class="c1"># ... recording id 3</span>
        <span class="s2">&quot;4/&quot;</span><span class="p">,</span>  <span class="c1"># ... recording id 4</span>
        <span class="s2">&quot;5/&quot;</span><span class="p">,</span>  <span class="c1"># ... recording id 5</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">dataset_dict</span><span class="p">,</span> <span class="n">value_counts</span> <span class="o">=</span> <span class="n">preprocess</span><span class="o">.</span><span class="n">prepare_dataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="c1"># good if you have large variation in number of data points for each label</span>
    <span class="n">equalize_data_within_splits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># set seed to get a reproducible data split</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># You can print the value_counts dataframe to see how many audio clips of each label</span>
<span class="c1"># (speaker) are present in each data subset.</span>
<span class="n">value_counts</span>
</pre></div>
</div>
</section>
</section>
<section id="model-training-and-evaluation">
<h3>Model Training and Evaluation<a class="headerlink" href="#model-training-and-evaluation" title="Permalink to this heading">#</a></h3>
<p>Once you have your dataset prepared and available, you can provide it directly to the
training function to begin training a new model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">eval_model</span></code> function will store a filed called <code class="docutils literal notranslate"><span class="pre">results.md</span></code> with the accuracy,
precision, and recall of the model and additionally store a file called
<code class="docutils literal notranslate"><span class="pre">validation-confusion.png</span></code> which is a
<a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>.</p>
<section id="id3">
<h4>Notes<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The model (and evaluation metrics) will be stored in a new directory called
<code class="docutils literal notranslate"><span class="pre">trained-speakerbox</span></code> <em>(parametrizable)</em>.</p></li>
<li><p>Training time depends on how much data you have annotated and provided.</p></li>
<li><p>It is recommended to train with an NVidia GPU with CUDA available to speed up
the training process.</p></li>
<li><p>Speakerbox has only been tested on English-language audio and the base model for
fine-tuning was trained on English-language audio. We provide no guarantees as to
it’s effectiveness on non-English-language audio. If you try Speakerbox on with
non-English-language audio, please let us know!</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox</span> <span class="kn">import</span> <span class="n">train</span><span class="p">,</span> <span class="n">eval_model</span>

<span class="c1"># dataset_dict comes from previous preparation step</span>
<span class="n">train</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">)</span>

<span class="n">eval_model</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="model-inference">
<h2>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this heading">#</a></h2>
<p>Once you have a trained model, you can use it against a new audio file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">speakerbox</span> <span class="kn">import</span> <span class="n">apply</span>

<span class="n">annotation</span> <span class="o">=</span> <span class="n">apply</span><span class="p">(</span>
    <span class="s2">&quot;new-audio.wav&quot;</span><span class="p">,</span>
    <span class="s2">&quot;path-to-my-model-directory/&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The apply function returns a
<a class="reference external" href="http://pyannote.github.io/pyannote-core/structure.html#annotation">pyannote.core.Annotation</a>.</p>
</section>
<section id="development">
<h2>Development<a class="headerlink" href="#development" title="Permalink to this heading">#</a></h2>
<p>See <a class="reference external" href="CONTRIBUTING.md">CONTRIBUTING.md</a> for information related to developing the code.</p>
</section>
<section id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this heading">#</a></h2>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">Brown2023</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.21105/joss.05132}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://doi.org/10.21105/joss.05132}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The Open Journal}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{8}</span><span class="p">,</span>
<span class="w">    </span><span class="na">number</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{83}</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{5132}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Eva Maxfield Brown and To Huynh and Nicholas Weber}</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Speakerbox: Few-Shot Learning for Speaker Identification with Transformers}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Journal of Open Source Software}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>MIT License</strong></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="installation.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Installation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    </body>
</html>